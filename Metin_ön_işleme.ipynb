{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9385850",
   "metadata": {},
   "source": [
    "# 1.Python ile veri temizleme:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd2feb",
   "metadata": {},
   "source": [
    "Metindeki fazla boşlukları kaldırma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503becda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:Hello,   Milenium World!  \n",
      "\n",
      "Cleaned Text:Hello, Milenium World!\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello,   Milenium World!  \"\n",
    "\n",
    "cleaned_text = \" \".join(text.split())\n",
    "print(f\"Text:{text}\\n\")\n",
    "print(f\"Cleaned Text:{cleaned_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef75ad",
   "metadata": {},
   "source": [
    "Büyük Harfleri küçük harflere çevirme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "219eb0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:Hello, My Dear I LOVE YOU very much\n",
      "\n",
      "Cleaned Text:hello, my dear i love you very much\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, My Dear I LOVE YOU very much\"\n",
    "\n",
    "cleaned_text = text.lower()\n",
    "print(f\"Text:{text}\\n\")\n",
    "print(f\"Cleaned Text:{cleaned_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b166a7b",
   "metadata": {},
   "source": [
    "Noktalama işaretleri kaldırma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0968b3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:Hello, my friend! What do you want?\n",
      "\n",
      "Cleaned Text:Hello my friend What do you want\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"Hello, my friend! What do you want?\"\n",
    "\n",
    "cleaned_text = text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "print(f\"Text:{text}\\n\")\n",
    "print(f\"Cleaned Text:{cleaned_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffffbf",
   "metadata": {},
   "source": [
    "Özel karakter kaldırma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ac8c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:Hello, progress of my education is 50% #Iprevail\n",
      "\n",
      "Cleaned Text:Hello progress of my education is 50 Iprevail\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, progress of my education is 50% #Iprevail\"\n",
    "\n",
    "cleaned_text = re.sub(r\"[^A-Za-z0-9\\s]\",\"\",text)\n",
    "print(f\"Text:{text}\\n\")\n",
    "print(f\"Cleaned Text:{cleaned_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04bb70",
   "metadata": {},
   "source": [
    "Yazım hatalarını düzelt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11e3aa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in c:\\users\\namkodx\\appdata\\roaming\\python\\python311\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\namkodx\\appdata\\roaming\\python\\python311\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e0b9ae8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:Hellio, Mye Languige is Turkish\n",
      "\n",
      "Cleaned Text:Hello, Eye Language is Turkish\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"Hellio, Mye Languige is Turkish\"\n",
    "cleaned_text = TextBlob(text).correct()\n",
    "\n",
    "print(f\"Text:{text}\\n\")\n",
    "print(f\"Cleaned Text:{cleaned_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6f83d",
   "metadata": {},
   "source": [
    "Okadar usta şekilde yapıları düzeltmiyor biraz zorlu ve sıkıntılı işlemler bunlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112950c1",
   "metadata": {},
   "source": [
    "Html ve url etiketlerini kaldırma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "892e51ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:Hellio, Mye Languige is Turkish\n",
      "\n",
      "Cleaned Text:Hello, World!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_text = \"<div><span>Hello, World!<span></div>\"\n",
    "cleaned_text = BeautifulSoup(html_text,\"html.parser\").get_text()\n",
    "\n",
    "print(f\"Text:{text}\\n\")\n",
    "print(f\"Cleaned Text:{cleaned_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebae1bb",
   "metadata": {},
   "source": [
    "# 2.Python ile Tokenizasyon:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09caa58",
   "metadata": {},
   "source": [
    "Tokenizasyon için punkt yüklemeliyiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67a7ae06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:Hello, World! I am fine. Are you good?\n",
      "\n",
      "Word Tokens:['Hello', ',', 'World', '!', 'I', 'am', 'fine', '.', 'Are', 'you', 'good', '?']\n",
      "Sentence Tokens:['Hello, World!', 'I am fine.', 'Are you good?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NamKodX\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")   \n",
    "\n",
    "text = \"Hello, World! I am fine. Are you good?\"\n",
    "\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "sentence_tokens = nltk.sent_tokenize(text)\n",
    "\n",
    "print(f\"Text:{text}\\n\")\n",
    "print(f\"Word Tokens:{word_tokens}\")\n",
    "print(f\"Sentence Tokens:{sentence_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22097b3a",
   "metadata": {},
   "source": [
    "# 3.Kök ve Gövde Analizi:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edce3518",
   "metadata": {},
   "source": [
    "lematization için wordnet yüklemeliyiz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd82d27d",
   "metadata": {},
   "source": [
    "Stemming işlemi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a8a7408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words Stems:['swim', 'hike', 'dive', 'ran', 'run', 'see', 'look', 'look']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NamKodX\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.stem import PorterStemmer    # Stemmer fonksiyonu\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"swimming\",\"hiking\",\"diving\",\"ran\",\"runs\",\"see\",\"look\",\"looking\"]\n",
    "\n",
    "stems = [stemmer.stem(w) for w in words]  # Kelimelerin stemlerini buluyoruz.\n",
    "\n",
    "print(f\"Words Stems:{stems}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f4681",
   "metadata": {},
   "source": [
    "Lematization işlemi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c49e741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words Lematization:['swimming', 'hiking', 'diving', 'ran', 'run', 'see', 'look', 'looking', 'go', 'went', 'took']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lematizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"swimming\",\"hiking\",\"diving\",\"ran\",\"runs\",\"see\",\"look\",\"looking\",\"go\",\"went\",\"took\"]\n",
    "\n",
    "lematization = [lematizer.lemmatize(w) for w in words]\n",
    "print(f\"Words Lematization:{lematization}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4db28f",
   "metadata": {},
   "source": [
    "# 4.Python ile durdurma kelimeleri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01b928bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NamKodX\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d035092",
   "metadata": {},
   "source": [
    "İngilizce için stop words analizi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f098701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'own', 'o', 'theirs', 'what', \"don't\", 'y', 'both', 'did', 've', 'll', 'shouldn', 'we', \"shouldn't\", \"should've\", 'you', \"they'd\", 'before', 't', 'such', 'doesn', 'do', \"he'll\", 'out', 'won', \"i'll\", \"wouldn't\", 'haven', 'ours', \"he's\", 'should', 'all', 'themselves', 're', \"hadn't\", 'had', 'by', 'i', 'herself', \"we're\", 'why', 'below', 'hasn', \"she's\", 'on', \"haven't\", 'into', 'is', 'when', \"i'm\", 'd', \"weren't\", 'than', 'weren', 'having', \"they're\", 'if', 'm', \"they'll\", 'aren', 'doing', 'as', 'this', 'ma', 'itself', 'some', 'it', 'be', \"you're\", 'shan', \"you've\", 'those', 'ain', \"couldn't\", 'hadn', 'him', 'again', 'couldn', 's', 'over', 'which', \"aren't\", \"doesn't\", 'their', 'more', 'too', 'its', 'from', 'so', 'don', \"she'd\", 'once', 'your', 'how', 'few', \"we'd\", 'because', 'his', 'are', 'has', 'there', 'off', 'wouldn', 'at', 'does', 'each', 'or', \"you'll\", 'mustn', 'only', \"i'd\", 'and', 'hers', \"i've\", 'that', 'himself', 'myself', 'where', 'a', 'during', 'mightn', \"that'll\", 'any', \"it's\", 'most', 'me', 'to', 'her', 'between', 'yourselves', 'of', 'just', 'in', 'he', 'isn', 'other', 'while', 'not', 'can', 'the', 'about', 'here', 'nor', 'an', 'they', 'after', 'them', \"didn't\", 'whom', 'needn', 'wasn', 'now', \"won't\", \"it'll\", \"we've\", 'with', 'same', 'who', 'but', \"shan't\", \"it'd\", 'down', 'very', \"they've\", 'she', 'further', 'being', 'were', \"you'd\", \"he'd\", 'our', 'through', 'will', 'above', \"mustn't\", 'yourself', \"she'll\", 'am', \"needn't\", 'was', 'been', 'up', 'these', \"we'll\", 'against', 'for', 'my', 'no', 'ourselves', 'yours', \"mightn't\", 'then', \"isn't\", \"wasn't\", 'have', 'until', \"hasn't\", 'under', 'didn'}\n"
     ]
    }
   ],
   "source": [
    "stop_words_english = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(stop_words_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "329b6be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered text:['sentences', 'board', 'school', 'night', 'winter.']\n"
     ]
    }
   ],
   "source": [
    "text = \"There are more sentences on the board in the school at night of winter.\"\n",
    "text_list = text.split()\n",
    "\n",
    "filter_text = [word for word in text_list if word.lower() not in stop_words_english]\n",
    "\n",
    "print(f\"Filtered text:{filter_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519afef1",
   "metadata": {},
   "source": [
    "Türkçe stop words temizleme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8eff3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'çok', 'diye', 'hiç', 'nereye', 'nasıl', 'o', 'hem', 'niye', 'aslında', 'gibi', 'ki', 'hepsi', 'ile', 've', 'biri', 'ise', 'bazı', 'ne', 'az', 'birkaç', 'yani', 'mu', 'kez', 'her', 'defa', 'da', 'için', 'ama', 'mü', 'ya', 'şey', 'neden', 'niçin', 'veya', 'hep', 'nerde', 'sanki', 'daha', 'acaba', 'tüm', 'eğer', 'siz', 'şu', 'en', 'de', 'belki', 'birşey', 'biz', 'çünkü', 'nerede', 'bu', 'kim', 'mı'}\n"
     ]
    }
   ],
   "source": [
    "stop_words_turkish = set(stopwords.words(\"turkish\"))\n",
    "\n",
    "print(stop_words_turkish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75f8c0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered text:['Merhaba', 'çalışma', 'ortamı', 'kodlar', 'faydalı', 'sizler', 'uğraştık', 'yoksa', 'uğraşacaktık']\n"
     ]
    }
   ],
   "source": [
    "metin = \"Merhaba bu çalışma ortamı ve kodlar çok faydalı çünkü biz sizler için uğraştık yoksa kim için uğraşacaktık\"\n",
    "metin_list = metin.split()\n",
    "\n",
    "filtered_text_tr = [word for word in metin_list if word.lower() not in stop_words_turkish]\n",
    "\n",
    "print(f\"Filtered text:{filtered_text_tr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc5e13",
   "metadata": {},
   "source": [
    "kütüphanesiz stop words çıkarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36635fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered text:['Merhaba', 'çalışma', 'ortamı', 'kodlar', 'faydalı', 'sizler', 'uğraştık', 'yoksa', 'uğraşacaktık']\n"
     ]
    }
   ],
   "source": [
    "tr_stopwords = ['çok', 'diye', 'hiç', 'nereye', 'nasıl', 'o', 'hem', 'niye', 'aslında', 'gibi', 'ki', 'hepsi', 'ile', 've', 'biri', 'ise', 'bazı', 'ne', 'az', 'birkaç', 'yani', 'mu', 'kez', 'her', 'defa', 'da', 'için', 'ama', 'mü', 'ya', 'şey', 'neden', 'niçin', 'veya', 'hep', 'nerde', 'sanki', 'daha', 'acaba', 'tüm', 'eğer', 'siz', 'şu', 'en', 'de', 'belki', 'birşey', 'biz', 'çünkü', 'nerede', 'bu', 'kim', 'mı']\n",
    "\n",
    "metin = \"Merhaba bu çalışma ortamı ve kodlar çok faydalı çünkü biz sizler için uğraştık yoksa kim için uğraşacaktık\"\n",
    "metin_list = metin.split()\n",
    "\n",
    "filtered_text_tr = [word for word in metin_list if word.lower() not in tr_stopwords]\n",
    "\n",
    "print(f\"Filtered text:{filtered_text_tr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95fd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
