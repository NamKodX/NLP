{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa1d008f",
   "metadata": {},
   "source": [
    "# 1.N-Gram Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1268194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kütüphaneleri yükleyelim\n",
    "import nltk \n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d35147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Örnek veri seti oluşturma\n",
    "corpus = [\n",
    "    \"Doğal dil işleme, metinleri anlamlandırmak için kullanılan bir yapay zeka dalıdır.\",\n",
    "    \"Makine öğrenmesi, büyük veri setlerinden anlamlı sonuçlar çıkarmaya yardımcı olur.\",\n",
    "    \"Derin öğrenme, sinir ağları kullanarak karmaşık problemleri çözmeyi sağlar.\",\n",
    "    \"Python, veri bilimi ve yapay zeka uygulamaları için popüler bir programlama dilidir.\",\n",
    "    \"Kelime gömme yöntemleri, metinleri sayısal vektörlere dönüştürerek analiz etmeye olanak tanır.\",\n",
    "    \"CNN'ler, görüntü işleme alanında yaygın olarak kullanılan derin öğrenme modelleridir.\",\n",
    "    \"RNN'ler, ardışık veri ve doğal dil işleme problemlerinde güçlüdür.\",\n",
    "    \"Transformers modelleri, büyük dil modelleri için en son teknolojilerden biridir.\",\n",
    "    \"GPT gibi büyük dil modelleri, çok dilli metin işleme yeteneğine sahiptir.\",\n",
    "    \"Tokenization, metinleri küçük parçalara ayırarak işlenebilir hale getirir.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7b6b429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:\n",
      " [['doğal', 'dil', 'işleme', ',', 'metinleri', 'anlamlandırmak', 'için', 'kullanılan', 'bir', 'yapay', 'zeka', 'dalıdır', '.'], ['makine', 'öğrenmesi', ',', 'büyük', 'veri', 'setlerinden', 'anlamlı', 'sonuçlar', 'çıkarmaya', 'yardımcı', 'olur', '.'], ['derin', 'öğrenme', ',', 'sinir', 'ağları', 'kullanarak', 'karmaşık', 'problemleri', 'çözmeyi', 'sağlar', '.'], ['python', ',', 'veri', 'bilimi', 've', 'yapay', 'zeka', 'uygulamaları', 'için', 'popüler', 'bir', 'programlama', 'dilidir', '.'], ['kelime', 'gömme', 'yöntemleri', ',', 'metinleri', 'sayısal', 'vektörlere', 'dönüştürerek', 'analiz', 'etmeye', 'olanak', 'tanır', '.'], [\"cnn'ler\", ',', 'görüntü', 'işleme', 'alanında', 'yaygın', 'olarak', 'kullanılan', 'derin', 'öğrenme', 'modelleridir', '.'], [\"rnn'ler\", ',', 'ardışık', 'veri', 've', 'doğal', 'dil', 'işleme', 'problemlerinde', 'güçlüdür', '.'], ['transformers', 'modelleri', ',', 'büyük', 'dil', 'modelleri', 'için', 'en', 'son', 'teknolojilerden', 'biridir', '.'], ['gpt', 'gibi', 'büyük', 'dil', 'modelleri', ',', 'çok', 'dilli', 'metin', 'işleme', 'yeteneğine', 'sahiptir', '.'], ['tokenization', ',', 'metinleri', 'küçük', 'parçalara', 'ayırarak', 'işlenebilir', 'hale', 'getirir', '.']]\n"
     ]
    }
   ],
   "source": [
    "tokens = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "print(\"tokens:\\n\",tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a415940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Freq:\n",
      " Counter({(',', 'metinleri'): 3, ('doğal', 'dil'): 2, ('dil', 'işleme'): 2, ('yapay', 'zeka'): 2, (',', 'büyük'): 2, ('derin', 'öğrenme'): 2, ('modelleri', ','): 2, ('büyük', 'dil'): 2, ('dil', 'modelleri'): 2, ('işleme', ','): 1, ('metinleri', 'anlamlandırmak'): 1, ('anlamlandırmak', 'için'): 1, ('için', 'kullanılan'): 1, ('kullanılan', 'bir'): 1, ('bir', 'yapay'): 1, ('zeka', 'dalıdır'): 1, ('dalıdır', '.'): 1, ('makine', 'öğrenmesi'): 1, ('öğrenmesi', ','): 1, ('büyük', 'veri'): 1, ('veri', 'setlerinden'): 1, ('setlerinden', 'anlamlı'): 1, ('anlamlı', 'sonuçlar'): 1, ('sonuçlar', 'çıkarmaya'): 1, ('çıkarmaya', 'yardımcı'): 1, ('yardımcı', 'olur'): 1, ('olur', '.'): 1, ('öğrenme', ','): 1, (',', 'sinir'): 1, ('sinir', 'ağları'): 1, ('ağları', 'kullanarak'): 1, ('kullanarak', 'karmaşık'): 1, ('karmaşık', 'problemleri'): 1, ('problemleri', 'çözmeyi'): 1, ('çözmeyi', 'sağlar'): 1, ('sağlar', '.'): 1, ('python', ','): 1, (',', 'veri'): 1, ('veri', 'bilimi'): 1, ('bilimi', 've'): 1, ('ve', 'yapay'): 1, ('zeka', 'uygulamaları'): 1, ('uygulamaları', 'için'): 1, ('için', 'popüler'): 1, ('popüler', 'bir'): 1, ('bir', 'programlama'): 1, ('programlama', 'dilidir'): 1, ('dilidir', '.'): 1, ('kelime', 'gömme'): 1, ('gömme', 'yöntemleri'): 1, ('yöntemleri', ','): 1, ('metinleri', 'sayısal'): 1, ('sayısal', 'vektörlere'): 1, ('vektörlere', 'dönüştürerek'): 1, ('dönüştürerek', 'analiz'): 1, ('analiz', 'etmeye'): 1, ('etmeye', 'olanak'): 1, ('olanak', 'tanır'): 1, ('tanır', '.'): 1, (\"cnn'ler\", ','): 1, (',', 'görüntü'): 1, ('görüntü', 'işleme'): 1, ('işleme', 'alanında'): 1, ('alanında', 'yaygın'): 1, ('yaygın', 'olarak'): 1, ('olarak', 'kullanılan'): 1, ('kullanılan', 'derin'): 1, ('öğrenme', 'modelleridir'): 1, ('modelleridir', '.'): 1, (\"rnn'ler\", ','): 1, (',', 'ardışık'): 1, ('ardışık', 'veri'): 1, ('veri', 've'): 1, ('ve', 'doğal'): 1, ('işleme', 'problemlerinde'): 1, ('problemlerinde', 'güçlüdür'): 1, ('güçlüdür', '.'): 1, ('transformers', 'modelleri'): 1, ('modelleri', 'için'): 1, ('için', 'en'): 1, ('en', 'son'): 1, ('son', 'teknolojilerden'): 1, ('teknolojilerden', 'biridir'): 1, ('biridir', '.'): 1, ('gpt', 'gibi'): 1, ('gibi', 'büyük'): 1, (',', 'çok'): 1, ('çok', 'dilli'): 1, ('dilli', 'metin'): 1, ('metin', 'işleme'): 1, ('işleme', 'yeteneğine'): 1, ('yeteneğine', 'sahiptir'): 1, ('sahiptir', '.'): 1, ('tokenization', ','): 1, ('metinleri', 'küçük'): 1, ('küçük', 'parçalara'): 1, ('parçalara', 'ayırarak'): 1, ('ayırarak', 'işlenebilir'): 1, ('işlenebilir', 'hale'): 1, ('hale', 'getirir'): 1, ('getirir', '.'): 1})\n"
     ]
    }
   ],
   "source": [
    "# Bigram alalım\n",
    "bigrams = []\n",
    "\n",
    "for token_list in tokens:\n",
    "    bigrams.extend(list(ngrams(token_list,2)))\n",
    "    \n",
    "bigram_freq = Counter(bigrams)\n",
    "\n",
    "print(\"Bigram Freq:\\n\",bigram_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16ddd5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Freq:\n",
      " Counter({(',', 'metinleri'): 3, ('doğal', 'dil'): 2, ('dil', 'işleme'): 2, ('yapay', 'zeka'): 2, (',', 'büyük'): 2, ('derin', 'öğrenme'): 2, ('modelleri', ','): 2, ('büyük', 'dil'): 2, ('dil', 'modelleri'): 2, ('işleme', ','): 1, ('metinleri', 'anlamlandırmak'): 1, ('anlamlandırmak', 'için'): 1, ('için', 'kullanılan'): 1, ('kullanılan', 'bir'): 1, ('bir', 'yapay'): 1, ('zeka', 'dalıdır'): 1, ('dalıdır', '.'): 1, ('makine', 'öğrenmesi'): 1, ('öğrenmesi', ','): 1, ('büyük', 'veri'): 1, ('veri', 'setlerinden'): 1, ('setlerinden', 'anlamlı'): 1, ('anlamlı', 'sonuçlar'): 1, ('sonuçlar', 'çıkarmaya'): 1, ('çıkarmaya', 'yardımcı'): 1, ('yardımcı', 'olur'): 1, ('olur', '.'): 1, ('öğrenme', ','): 1, (',', 'sinir'): 1, ('sinir', 'ağları'): 1, ('ağları', 'kullanarak'): 1, ('kullanarak', 'karmaşık'): 1, ('karmaşık', 'problemleri'): 1, ('problemleri', 'çözmeyi'): 1, ('çözmeyi', 'sağlar'): 1, ('sağlar', '.'): 1, ('python', ','): 1, (',', 'veri'): 1, ('veri', 'bilimi'): 1, ('bilimi', 've'): 1, ('ve', 'yapay'): 1, ('zeka', 'uygulamaları'): 1, ('uygulamaları', 'için'): 1, ('için', 'popüler'): 1, ('popüler', 'bir'): 1, ('bir', 'programlama'): 1, ('programlama', 'dilidir'): 1, ('dilidir', '.'): 1, ('kelime', 'gömme'): 1, ('gömme', 'yöntemleri'): 1, ('yöntemleri', ','): 1, ('metinleri', 'sayısal'): 1, ('sayısal', 'vektörlere'): 1, ('vektörlere', 'dönüştürerek'): 1, ('dönüştürerek', 'analiz'): 1, ('analiz', 'etmeye'): 1, ('etmeye', 'olanak'): 1, ('olanak', 'tanır'): 1, ('tanır', '.'): 1, (\"cnn'ler\", ','): 1, (',', 'görüntü'): 1, ('görüntü', 'işleme'): 1, ('işleme', 'alanında'): 1, ('alanında', 'yaygın'): 1, ('yaygın', 'olarak'): 1, ('olarak', 'kullanılan'): 1, ('kullanılan', 'derin'): 1, ('öğrenme', 'modelleridir'): 1, ('modelleridir', '.'): 1, (\"rnn'ler\", ','): 1, (',', 'ardışık'): 1, ('ardışık', 'veri'): 1, ('veri', 've'): 1, ('ve', 'doğal'): 1, ('işleme', 'problemlerinde'): 1, ('problemlerinde', 'güçlüdür'): 1, ('güçlüdür', '.'): 1, ('transformers', 'modelleri'): 1, ('modelleri', 'için'): 1, ('için', 'en'): 1, ('en', 'son'): 1, ('son', 'teknolojilerden'): 1, ('teknolojilerden', 'biridir'): 1, ('biridir', '.'): 1, ('gpt', 'gibi'): 1, ('gibi', 'büyük'): 1, (',', 'çok'): 1, ('çok', 'dilli'): 1, ('dilli', 'metin'): 1, ('metin', 'işleme'): 1, ('işleme', 'yeteneğine'): 1, ('yeteneğine', 'sahiptir'): 1, ('sahiptir', '.'): 1, ('tokenization', ','): 1, ('metinleri', 'küçük'): 1, ('küçük', 'parçalara'): 1, ('parçalara', 'ayırarak'): 1, ('ayırarak', 'işlenebilir'): 1, ('işlenebilir', 'hale'): 1, ('hale', 'getirir'): 1, ('getirir', '.'): 1})\n"
     ]
    }
   ],
   "source": [
    "# Trigram alalım\n",
    "trigrams = []\n",
    "\n",
    "for token_list in tokens:\n",
    "    trigrams.extend(list(ngrams(token_list,3)))\n",
    "    \n",
    "trigram_freq = Counter(bigrams)\n",
    "\n",
    "print(\"Trigram Freq:\\n\",trigram_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0efd4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Doğal Dil\" bigramından sonra \"İşleme\" gelme olasılık hesaplama\n",
    "bigram = (\"doğal\",\"dil\") # hedef bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a2d4675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doğal dil den sonra işleme gelme olasılığı:\n",
      " 0.0\n"
     ]
    }
   ],
   "source": [
    "# \"Doğal Dil işleme\" olma olasılık\n",
    "prob_isleme = trigram_freq[(\"doğal\",\"dil\",\"işleme\")]/bigram_freq[bigram]\n",
    "print(\"Doğal dil den sonra işleme gelme olasılığı:\\n\",prob_isleme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6d71a",
   "metadata": {},
   "source": [
    "# 2.Hidden Markov Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c572701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kütüphaneleri yükleme\n",
    "import nltk\n",
    "from nltk.tag import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c462611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Örnek training dataları\n",
    "train_data = [\n",
    "    [(\"Merhaba\", \"UH\"), (\"dünya\", \"NN\"), (\"!\", \"PUNC\")],\n",
    "    [(\"Bugün\", \"NN\"), (\"hava\", \"NN\"), (\"çok\", \"RB\"), (\"güzel\", \"JJ\"), (\".\", \"PUNC\")],\n",
    "    [(\"Ali\", \"NNP\"), (\"okula\", \"NN\"), (\"gitti\", \"VB\"), (\".\", \"PUNC\")],\n",
    "    [(\"Ben\", \"PRP\"), (\"sabah\", \"NN\"), (\"kahve\", \"NN\"), (\"içtim\", \"VB\"), (\".\", \"PUNC\")],\n",
    "    [(\"Araba\", \"NN\"), (\"çok\", \"RB\"), (\"hızlı\", \"JJ\"), (\".\", \"PUNC\")],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efff8752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HiddenMarkovModelTagger 8 states and 17 output symbols>\n"
     ]
    }
   ],
   "source": [
    "# Model train\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = trainer.train(train_data)\n",
    "print(hmm_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b91354e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bugün', 'sabah', 'kahve', 'içtim']\n"
     ]
    }
   ],
   "source": [
    "# Yeni bir cümle oluşturup içerisinde bulunan her sözcük türünü etiketle\n",
    "test_sentence = \"Bugün sabah kahve içtim\".split()\n",
    "print(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1aa31edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeni Cümle: [('Bugün', 'NN'), ('sabah', 'NN'), ('kahve', 'NN'), ('içtim', 'VB')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NamKodX\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tag\\hmm.py:333: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "C:\\Users\\NamKodX\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tag\\hmm.py:335: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "C:\\Users\\NamKodX\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tag\\hmm.py:331: RuntimeWarning: overflow encountered in cast\n",
      "  P[i] = self._priors.logprob(si)\n"
     ]
    }
   ],
   "source": [
    "tags = hmm_tagger.tag(test_sentence)\n",
    "print(f\"Yeni Cümle: {tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7eea74",
   "metadata": {},
   "source": [
    "Farklı bir örnek yapalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61353695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kütüphaneleri yükleme\n",
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e1499d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\NamKodX\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veri setini içeri aktarma\n",
    "nltk.download(\"conll2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27144f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = conll2000.tagged_sents(\"train.txt\")\n",
    "test_data = conll2000.tagged_sents(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d155fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: [[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pound', 'NN'), ('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB'), ('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN'), ('if', 'IN'), ('trade', 'NN'), ('figures', 'NNS'), ('for', 'IN'), ('September', 'NNP'), (',', ','), ('due', 'JJ'), ('for', 'IN'), ('release', 'NN'), ('tomorrow', 'NN'), (',', ','), ('fail', 'VB'), ('to', 'TO'), ('show', 'VB'), ('a', 'DT'), ('substantial', 'JJ'), ('improvement', 'NN'), ('from', 'IN'), ('July', 'NNP'), ('and', 'CC'), ('August', 'NNP'), (\"'s\", 'POS'), ('near-record', 'JJ'), ('deficits', 'NNS'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Data: {train_data[:1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23098f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train hidden markov model\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = trainer.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34b04efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NamKodX\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tag\\hmm.py:333: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "C:\\Users\\NamKodX\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tag\\hmm.py:335: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "C:\\Users\\NamKodX\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tag\\hmm.py:331: RuntimeWarning: overflow encountered in cast\n",
      "  P[i] = self._priors.logprob(si)\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I like going to school\".split()\n",
    "tags = hmm_tagger.tag(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c5f5a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeni Cümle: [('I', 'PRP'), ('like', 'IN'), ('going', 'VBG'), ('to', 'TO'), ('school', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Yeni Cümle: {tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfb45c",
   "metadata": {},
   "source": [
    "# 3.Maximum Entropy Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818015d",
   "metadata": {},
   "source": [
    "Sınıflandırma problemi olarak duygu analizi yapalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03bacca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import MaxentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae95cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setini tanımlayalım\n",
    "train_data = [\n",
    "    ({\"love\": True, \"amazing\": True, \"happy\": True, \"terrible\": False}, \"positive\"),\n",
    "    ({\"hate\": True, \"terrible\": True}, \"negative\"),\n",
    "    ({\"joy\": True, \"happy\": True, \"hate\": False}, \"positive\"),\n",
    "    ({\"sad\": True, \"depressed\": True, \"love\": False}, \"negative\"),\n",
    "    ({\"exciting\": True, \"fantastic\": True, \"great\": True}, \"positive\"),\n",
    "    ({\"boring\": True, \"dull\": True, \"terrible\": True}, \"negative\"),\n",
    "    ({\"wonderful\": True, \"awesome\": True, \"joy\": True}, \"positive\"),\n",
    "    ({\"angry\": True, \"bad\": True, \"hate\": True}, \"negative\"),\n",
    "    ({\"fun\": True, \"smile\": True, \"nice\": True}, \"positive\"),\n",
    "    ({\"disappointed\": True, \"awful\": True, \"sad\": True}, \"negative\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09aac9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.500\n",
      "             2          -0.40802        1.000\n",
      "             3          -0.29060        1.000\n",
      "             4          -0.22599        1.000\n",
      "             5          -0.18500        1.000\n",
      "             6          -0.15665        1.000\n",
      "             7          -0.13585        1.000\n",
      "             8          -0.11994        1.000\n",
      "             9          -0.10737        1.000\n",
      "            10          -0.09720        1.000\n",
      "            11          -0.08878        1.000\n",
      "            12          -0.08171        1.000\n",
      "            13          -0.07568        1.000\n",
      "            14          -0.07048        1.000\n",
      "            15          -0.06595        1.000\n",
      "            16          -0.06197        1.000\n",
      "            17          -0.05844        1.000\n",
      "            18          -0.05530        1.000\n",
      "            19          -0.05247        1.000\n",
      "            20          -0.04992        1.000\n",
      "            21          -0.04760        1.000\n",
      "            22          -0.04549        1.000\n",
      "            23          -0.04356        1.000\n",
      "            24          -0.04179        1.000\n",
      "            25          -0.04016        1.000\n",
      "            26          -0.03865        1.000\n",
      "            27          -0.03724        1.000\n",
      "            28          -0.03594        1.000\n",
      "            29          -0.03473        1.000\n",
      "            30          -0.03359        1.000\n",
      "            31          -0.03253        1.000\n",
      "            32          -0.03153        1.000\n",
      "            33          -0.03059        1.000\n",
      "            34          -0.02970        1.000\n",
      "            35          -0.02887        1.000\n",
      "            36          -0.02808        1.000\n",
      "            37          -0.02733        1.000\n",
      "            38          -0.02662        1.000\n",
      "            39          -0.02595        1.000\n",
      "            40          -0.02531        1.000\n",
      "            41          -0.02470        1.000\n",
      "            42          -0.02412        1.000\n",
      "            43          -0.02357        1.000\n",
      "            44          -0.02304        1.000\n",
      "            45          -0.02253        1.000\n",
      "            46          -0.02205        1.000\n",
      "            47          -0.02159        1.000\n",
      "            48          -0.02114        1.000\n",
      "            49          -0.02072        1.000\n",
      "            50          -0.02031        1.000\n",
      "            51          -0.01991        1.000\n",
      "            52          -0.01953        1.000\n",
      "            53          -0.01917        1.000\n",
      "            54          -0.01882        1.000\n",
      "            55          -0.01848        1.000\n",
      "            56          -0.01815        1.000\n",
      "            57          -0.01784        1.000\n",
      "            58          -0.01753        1.000\n",
      "            59          -0.01724        1.000\n",
      "            60          -0.01695        1.000\n",
      "            61          -0.01668        1.000\n",
      "            62          -0.01641        1.000\n",
      "            63          -0.01615        1.000\n",
      "            64          -0.01590        1.000\n",
      "            65          -0.01566        1.000\n",
      "            66          -0.01543        1.000\n",
      "            67          -0.01520        1.000\n",
      "            68          -0.01498        1.000\n",
      "            69          -0.01476        1.000\n",
      "            70          -0.01455        1.000\n",
      "            71          -0.01435        1.000\n",
      "            72          -0.01415        1.000\n",
      "            73          -0.01396        1.000\n",
      "            74          -0.01377        1.000\n",
      "            75          -0.01359        1.000\n",
      "            76          -0.01341        1.000\n",
      "            77          -0.01324        1.000\n",
      "            78          -0.01307        1.000\n",
      "            79          -0.01291        1.000\n",
      "            80          -0.01275        1.000\n",
      "            81          -0.01259        1.000\n",
      "            82          -0.01244        1.000\n",
      "            83          -0.01229        1.000\n",
      "            84          -0.01214        1.000\n",
      "            85          -0.01200        1.000\n",
      "            86          -0.01186        1.000\n",
      "            87          -0.01173        1.000\n",
      "            88          -0.01159        1.000\n",
      "            89          -0.01146        1.000\n",
      "            90          -0.01134        1.000\n",
      "            91          -0.01121        1.000\n",
      "            92          -0.01109        1.000\n",
      "            93          -0.01097        1.000\n",
      "            94          -0.01086        1.000\n",
      "            95          -0.01074        1.000\n",
      "            96          -0.01063        1.000\n",
      "            97          -0.01052        1.000\n",
      "            98          -0.01042        1.000\n",
      "            99          -0.01031        1.000\n",
      "         Final          -0.01021        1.000\n"
     ]
    }
   ],
   "source": [
    "# maximum entropy modeline göre eğitme yapalım\n",
    "classifier = MaxentClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b118a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yeni cümle ile test edelim\n",
    "test_sentence = \"I do not like this game\"\n",
    "features = {word : (word in test_sentence.lower().split()) for word in [\"love\",\"amazing\",\"terrible\",\"happy\",\"joy\",\"depressed\",\"sad\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f941e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': False, 'amazing': False, 'terrible': False, 'happy': False, 'joy': False, 'depressed': False, 'sad': False}\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "795f53b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I do not like this game\n",
      "Result: negative\n"
     ]
    }
   ],
   "source": [
    "label = classifier.classify(features)\n",
    "print(f\"Sentence: {test_sentence}\")\n",
    "print(f\"Result: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9b91a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
